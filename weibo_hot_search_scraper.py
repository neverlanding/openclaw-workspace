#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®åšçƒ­æœçˆ¬è™«è„šæœ¬
ç”¨äºå®šæœŸè·å–å¾®åšçƒ­æœä¿¡æ¯å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import requests
import json
import time
from bs4 import BeautifulSoup
import re
import csv
from datetime import datetime
import os


class WeiboHotSearchScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # å¾®åšçƒ­æœAPIæ¥å£
        self.api_url = "https://m.weibo.cn/api/container/getIndex?containerid=106003%2Btype%3D25%26t%3D3%26disable_hot%3D1%26filter_type%3Drealtimehot"
        self.hot_search_url = "https://s.weibo.com/top/summary"

    def get_hot_search_data(self):
        """
        è·å–å¾®åšçƒ­æœæ•°æ®
        """
        try:
            response = requests.get(self.api_url, headers=self.headers)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                data = response.json()
                
                if data['ok'] == 1:
                    cards = data['data']['cards']
                    
                    hot_searches = []
                    for card in cards:
                        if card['card_type'] == 'feed':
                            for item in card['card_group']:
                                hot_search_item = {}
                                
                                # æ ‡é¢˜
                                title = item.get('raw_title', '')
                                hot_search_item['title'] = title
                                
                                # æ’å
                                rank = item.get('rank', '')
                                hot_search_item['rank'] = rank
                                
                                # çƒ­åº¦å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
                                hot_score = item.get('hot_scheme', '')
                                hot_search_item['hot_score'] = hot_score
                                
                                # é“¾æ¥
                                scheme = item.get('scheme', '')
                                hot_search_item['link'] = scheme
                                
                                # æ˜¯å¦æ˜¯çƒ­é—¨è¯é¢˜
                                is_topic = item.get('is_topic_plus', 0)
                                hot_search_item['is_topic'] = is_topic
                                
                                hot_searches.append(hot_search_item)
                    
                    return hot_searches
                else:
                    print("APIè¿”å›é”™è¯¯:", data)
                    return None
            else:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            print(f"è·å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def get_hot_search_via_web(self):
        """
        é€šè¿‡ç½‘é¡µè·å–å¾®åšçƒ­æœï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        try:
            response = requests.get(self.hot_search_url, headers=self.headers)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾çƒ­æœåˆ—è¡¨
                items = soup.find_all('td', class_='td-02')
                
                hot_searches = []
                for i, item in enumerate(items, 1):
                    link_tag = item.find('a')
                    if link_tag:
                        title = link_tag.get_text().strip()
                        link = "https://s.weibo.com" + link_tag.get('href', '')
                        
                        hot_searches.append({
                            'rank': i,
                            'title': title,
                            'link': link
                        })
                
                return hot_searches
            else:
                print(f"ç½‘é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            print(f"è§£æç½‘é¡µæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def format_report(self, hot_searches):
        """
        æ ¼å¼åŒ–æŠ¥å‘Š
        """
        if not hot_searches:
            return "æœªèƒ½è·å–åˆ°å¾®åšçƒ­æœæ•°æ®"
        
        report = []
        report.append("=" * 60)
        report.append("å¾®åšå®æ—¶çƒ­æœæ¦œ")
        report.append(f"æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 60)
        
        for i, item in enumerate(hot_searches[:20], 1):  # åªæ˜¾ç¤ºå‰20æ¡
            rank = item.get('rank', i)
            title = item.get('title', 'N/A')
            link = item.get('link', '')
            
            # æ·»åŠ æ’åæ ·å¼
            if rank == 1:
                rank_str = f"ğŸ† {rank:2d}"
            elif rank == 2:
                rank_str = f"ğŸ¥ˆ {rank:2d}"
            elif rank == 3:
                rank_str = f"ğŸ¥‰ {rank:2d}"
            else:
                rank_str = f"  {rank:2d}"
                
            report.append(f"{rank_str}. {title}")
            if link:
                report.append(f"     é“¾æ¥: {link}")
            report.append("")
        
        report.append("=" * 60)
        report.append(f"å…±è·å– {len(hot_searches)} æ¡çƒ­æœä¿¡æ¯")
        report.append("=" * 60)
        
        return "\n".join(report)

    def save_to_csv(self, hot_searches, filename=None):
        """
        ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        """
        if not filename:
            filename = f"weibo_hot_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            fieldnames = ['rank', 'title', 'link', 'hot_score']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for item in hot_searches:
                writer.writerow(item)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

    def run(self):
        """
        è¿è¡Œçˆ¬è™«
        """
        print("æ­£åœ¨è·å–å¾®åšçƒ­æœæ•°æ®...")
        
        # å°è¯•ä½¿ç”¨APIæ–¹å¼è·å–æ•°æ®
        hot_searches = self.get_hot_search_data()
        
        # å¦‚æœAPIæ–¹å¼å¤±è´¥ï¼Œå°è¯•ç½‘é¡µæ–¹å¼
        if not hot_searches:
            print("APIæ–¹å¼å¤±è´¥ï¼Œå°è¯•ç½‘é¡µè§£ææ–¹å¼...")
            hot_searches = self.get_hot_search_via_web()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.format_report(hot_searches)
        print(report)
        
        # ä¿å­˜æ•°æ®
        if hot_searches:
            self.save_to_csv(hot_searches)
        
        return hot_searches


def main():
    scraper = WeiboHotSearchScraper()
    scraper.run()


if __name__ == "__main__":
    main()